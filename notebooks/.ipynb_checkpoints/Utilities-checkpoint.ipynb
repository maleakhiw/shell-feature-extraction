{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n",
    "**Author**: Maleakhi Agung Wijaya  \n",
    "**Email**: maw219@cam.ac.uk  \n",
    "**Description**: This file contains utility functions and constants used in other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain knowledge filenames\n",
    "all_domain_knowledge = \"../results/new_shell_comp.mat\"\n",
    "color_domain_knowledge = \"../results/shell_color.mat\"\n",
    "shape_domain_knowledge = \"../results/shell_shape.mat\"\n",
    "texture_domain_knowledge = \"../results/shell_texture.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW data folder\n",
    "SHELL_IMAGES_DATA = \"../data/shell_species_134_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_domain_knowledge_data(filepath):\n",
    "    \"\"\"\n",
    "    Given filepath, we load the data (features and label)\n",
    "    after preprocessing using domain knowledge.\n",
    "    \n",
    "    Note: for the domain knowledge feature extraction, we follow extraction\n",
    "    steps discussed by Zhang et al. \n",
    "    (https://www.nature.com/articles/s41597-019-0230-3.pdf). We adapted\n",
    "    the code from Matlab to Python.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load extracted feature\n",
    "    data = scipy.io.loadmat(filepath)\n",
    "    X = data[\"X\"]\n",
    "    y = data[\"Y\"]\n",
    "    \n",
    "    # Return the features and labels\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, cmap=\"coolwarm\"):\n",
    "    \"\"\"\n",
    "    Used to draw confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, cmap=cmap, interpolation=\"nearest\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\", fontsize=13)\n",
    "    plt.ylabel(\"Actual\", fontsize=13)\n",
    "    plt.tick_params(axis='both', which='both', bottom=False, \n",
    "                    top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_sklearn(classifier, param_grids, X, y, n_iter=5):\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation (for sklearn ML models).\n",
    "    \n",
    "    :param classifier: classifier of interest\n",
    "    :param param_grids: dictionary combination of parameters to be \n",
    "        tried for hyperparameter search.\n",
    "    :param X: the X feature\n",
    "    :param y: the y feature\n",
    "    :param n_iter: the number of iteration in the nested cv\n",
    "    \n",
    "    :return: list cv accuracy, aggregate confusion matrix, list of hyperparameter search result\n",
    "    \"\"\"\n",
    "    \n",
    "    list_acc = []\n",
    "    list_f1 = []\n",
    "    list_cv_results = []\n",
    "    list_cm = []\n",
    "    \n",
    "    ## Perform nested cross-validation (outer CV = monte-carlo based)\n",
    "    for _ in range(n_iter):\n",
    "        # Split into training and testing\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, \n",
    "                                                           shuffle=True, test_size=0.2)\n",
    "        \n",
    "        # Inner CV: search for the best model\n",
    "        clf = GridSearchCV(classifier, param_grids, cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        cv_results = clf.cv_results_\n",
    "        \n",
    "        # Evaluate on testing using the best model\n",
    "        y_pred = clf.best_estimator_.predict(X_test)\n",
    "        acc = accuracy_score(y_pred, y_test)\n",
    "        f1 = f1_score(y_pred, y_test, average=\"macro\")\n",
    "        cm = confusion_matrix(y_pred, y_test)\n",
    "        \n",
    "        # Store result\n",
    "        list_acc.append(acc)\n",
    "        list_cm.append(cm)\n",
    "        list_f1.append(f1)\n",
    "        list_cv_results.append(cv_results)\n",
    "    \n",
    "    return list_acc, list_cm, list_f1, list_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict_results():\n",
    "    # Generate dictionary used to store results for each feature set\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": [],\n",
    "        \"f1\": [],\n",
    "        \"cv_results\": [],\n",
    "        \"cm\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_x_y(filenames):\n",
    "    \"\"\"\n",
    "    Given the format from the dataset, this function split\n",
    "    the image into X and y respectively. Note that we combine\n",
    "    front and rear view of the shells as a single image.\n",
    "    \n",
    "    :param filenames: list of filenames inside the list with\n",
    "        format = species_idx_A/B.jpg\n",
    "        \n",
    "    :return X: the feature (tuple A, B)\n",
    "    :return y: the label\n",
    "    \"\"\"\n",
    "    \n",
    "    X_species = []\n",
    "    y_species = []\n",
    "    \n",
    "    for i in range(0, len(filenames), 2):\n",
    "        X_species.append((filenames[i], filenames[i+1]))\n",
    "\n",
    "        # Find the digit index\n",
    "        label = re.search(r\"\\d\", filenames[i])\n",
    "        idx_label = label.start() - 1\n",
    "        y_species.append(filenames[i][:idx_label])\n",
    "    \n",
    "    return X_species, y_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(grayscale_img):\n",
    "    \"\"\"\n",
    "    Apply mask to the grayscale image such that we only focus on\n",
    "    the shell. It should be noted however, that since the \n",
    "    background of the image is already black due to flood fill\n",
    "    algorithm, masking might not be necessary.\n",
    "    \"\"\"\n",
    "    \n",
    "    _, mask = cv2.threshold(grayscale_img, 1, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save and load pickle objects.\n",
    "\"\"\"\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_histograms(dictionary, vocabulary_size, X, feature_extractor=cv2.xfeatures2d.SIFT_create()):\n",
    "    \"\"\"\n",
    "    Build histogram depending on the vocabulary (dictionary)\n",
    "    and data given.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Storage\n",
    "    histograms = [] # list of bovw features for each image\n",
    "    \n",
    "    for x in tqdm(X):\n",
    "        X_a = x[0] # A position\n",
    "        X_b = x[1]\n",
    "\n",
    "        # Open the image in gray scale\n",
    "        image_a = cv2.imread(os.path.join(SHELL_IMAGES_DATA, X_a),\n",
    "                       cv2.IMREAD_GRAYSCALE)\n",
    "        image_b = cv2.imread(os.path.join(SHELL_IMAGES_DATA, X_b),\n",
    "                       cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        kp_a, descriptors_a = feature_extractor.detectAndCompute(image_a, None)\n",
    "        kp_b, descriptors_b = feature_extractor.detectAndCompute(image_b, None)\n",
    "\n",
    "        descriptors = np.concatenate((descriptors_a, descriptors_b))\n",
    "\n",
    "        # Calculate distance to each 200 vocab symbols\n",
    "        dist = cdist(descriptors, dictionary, \"euclidean\")\n",
    "\n",
    "        # get the symbolno for the closest symbol\n",
    "        cluster_assignment = np.argmin(dist, axis=1)\n",
    "\n",
    "        # Build histogram\n",
    "        features = np.zeros(vocabulary_size)\n",
    "        for assign in cluster_assignment:\n",
    "            features[assign] += 1\n",
    "\n",
    "        histograms.append(features)\n",
    "    \n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_histograms_brief(dictionary, vocabulary_size, X):\n",
    "    \"\"\"\n",
    "    Build histogram depending on the vocabulary (dictionary)\n",
    "    and data given.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Storage\n",
    "    histograms = [] # list of bovw features for each image\n",
    "    star = cv2.FeatureDetector_create(\"STAR\")\n",
    "    brief = cv2.DescriptorExtractor_create(\"BRIEF\")\n",
    "    \n",
    "    for x in tqdm(X):\n",
    "        X_a = x[0] # A position\n",
    "        X_b = x[1]\n",
    "\n",
    "        # Open the image in gray scale\n",
    "        image_a = cv2.imread(os.path.join(SHELL_IMAGES_DATA, X_a),\n",
    "                       cv2.IMREAD_GRAYSCALE)\n",
    "        image_b = cv2.imread(os.path.join(SHELL_IMAGES_DATA, X_b),\n",
    "                       cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        kp_a = star.detect(image_a, None)\n",
    "        kp_b = star.detect(image_b, None)\n",
    "        kp_a, descriptors_a = brief.compute(image_a, kp_a)\n",
    "        kp_b, descriptors_b = brief.compute(image_b, kp_b)\n",
    "\n",
    "        descriptors = np.concatenate((descriptors_a, descriptors_b))\n",
    "\n",
    "        # Calculate distance to each 200 vocab symbols\n",
    "        dist = cdist(descriptors, dictionary, \"euclidean\")\n",
    "\n",
    "        # get the symbolno for the closest symbol\n",
    "        cluster_assignment = np.argmin(dist, axis=1)\n",
    "\n",
    "        # Build histogram\n",
    "        features = np.zeros(vocabulary_size)\n",
    "        for assign in cluster_assignment:\n",
    "            features[assign] += 1\n",
    "\n",
    "        histograms.append(features)\n",
    "    \n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_canny_edge(path, sigma=0.33):\n",
    "    \"\"\"\n",
    "    Given an image path, we extract the edge using zero-parameter,\n",
    "    automatic Canny edge detector.\n",
    "    \n",
    "    :param path: the image path.\n",
    "    :param sigma: the tightness level of threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read image, convert it to grayscale, and apply Gaussian blue\n",
    "    image = cv2.imread(path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    \n",
    "    # Perform Canny detection (automatically)\n",
    "    # We automatically search for upper and lower threshold using\n",
    "    # the median of the blurred image.\n",
    "    v = np.median(blurred)\n",
    "    lower = int(max(0, (1.0-sigma) * v))\n",
    "    upper = int(min(255, (1.0+sigma) * v))\n",
    "    edge_img = cv2.Canny(blurred, lower, upper)\n",
    "    \n",
    "    return edge_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_canny(path, sigma=0.33):\n",
    "    \"\"\"\n",
    "    Given an image path, plot the edge found by canny.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process image using canny detector\n",
    "    edged_img = extract_canny_edge(path, sigma=sigma)\n",
    "    \n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(edged_img, cmap=\"gray\")\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
